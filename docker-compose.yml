services:
  mosquitto:
    build:
      context: ./mosquitto
      dockerfile: Dockerfile
    ports:
      - "1883:1883"
      - "9001:9001"
    volumes:
      - ./mosquitto/config:/mosquitto/config
      - ./mosquitto/data:/mosquitto/data
      - ./mosquitto/log:/mosquitto/log

  simulator:
    build:
      context: ./simulator
      dockerfile: Dockerfile
    environment:
      - MQTT_BROKER=mosquitto
    depends_on:
      - mosquitto

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    ports:
      - "3000:3000"

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - MQTT_BROKER=mosquitto
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=verystrongpassword
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=minio
      - MINIO_SECRET_KEY=minio123
      - SPARK_HOST=spark-master
      - SPARK_PORT=7077
    depends_on:
      - mosquitto
    volumes:
      - spark-events:/tmp/spark-events


# --- Spark cluster -------------------------------------------------
  spark-master:
    build:
      context: ./spark
      dockerfile: Dockerfile
    hostname: spark-master
    container_name: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_HOST=spark-master
      # --- (optional) security flags – comment out if not needed ----
      # - SPARK_RPC_AUTHENTICATION_ENABLED=yes
      # - SPARK_RPC_AUTHENTICATION_SECRET=mysparksecret
      # - SPARK_RPC_ENCRYPTION_ENABLED=yes
      # - SPARK_SSL_ENABLED=yes
    ports:
      - "7077:7077"   # cluster RPC
      - "8080:8080"   # master web-UI → http://localhost:8080
    volumes:
      - spark-events:/tmp/spark-events   # shared event-logs

  spark-worker:
    build:
      context: ./spark
      dockerfile: Dockerfile    # `deploy` only works with Swarm/K8s; use `--scale` instead.
    hostname: spark-worker-{{.Task.Slot}}   # nice names if you scale
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G             # adjust to your host
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_WEBUI_PORT=8081
    ports:
      - "8081:8081"   # worker web-UI → http://localhost:8081
    depends_on:
      - spark-master
    volumes:
      - spark-events:/tmp/spark-events

  spark-history:
    build:
      context: ./spark
      dockerfile: Dockerfile
    container_name: spark-history
    # no SPARK_MODE here ─ we launch the class by hand
    command: >
      /opt/bitnami/spark/bin/spark-class
      org.apache.spark.deploy.history.HistoryServer
    environment:
      # Where the workers & master write their logs
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=/tmp/spark-events
    volumes:
      - spark-events:/tmp/spark-events   # same shared volume
    ports:
      - "18080:18080"    # UI → http://localhost:18080
    depends_on:
      - spark-master
# -------------------------------------------------------------------

# ------------------------------------------------ MinIO object store
  minio:
    image: minio/minio:RELEASE.2025-04-22T22-12-26Z-cpuv1
    container_name: minio
    command: server /data --console-address ":9090"
    environment:
      - MINIO_ROOT_USER=minio
      - MINIO_ROOT_PASSWORD=minio123
    ports:
      - "9000:9000"   # S3 API  → http://localhost:9000
      - "9090:9090"   # Web UI → http://localhost:9090
    volumes:
      - minio-data:/data            # keeps objects between restarts

  # one-shot job that creates the bucket “lake” if it’s missing
  # minio-init:
  #   image: minio/mc
  #   depends_on: [minio]
  #   entrypoint: >
  #     /bin/sh -c "
  #       until mc alias set local http://minio:9000 minio minio123; do sleep 1; done &&
  #       mc mb -p local/lake || true &&
  #       mc policy set public local/lake;
  #       exit 0"
# -------------------------------------------------------------------


  neo4j:
    image: neo4j:5.17.0
    ports:
      - "7474:7474"   # HTTP interface (UI)
      - "7687:7687"   # Bolt protocol
    environment:
      - NEO4J_AUTH=neo4j/verystrongpassword  # update credentials as required
      - NEO4J_PLUGINS=["apoc", "graph-data-science", "n10s"]
      # - NEO4J_dbms_security_procedures_unrestricted=apoc.*
      # - apoc.import.file.enabled=true
    # volumes:
    #   - ./frontend/public/ontology.owl:/var/lib/neo4j/import/ontology.owl:ro

  # neo4j_importer:
  #   image: neo4j:latest
  #   depends_on:
  #     - neo4j
  #   volumes:
  #     - ./neo4j/init_ontology.sh:/init_ontology.sh:ro
  #   entrypoint: ["bash", "/init_ontology.sh"]

volumes:
  spark-events: {}
  minio-data: {}
